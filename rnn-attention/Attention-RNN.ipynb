{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>xml</th>\n",
       "      <th>closure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;h1&gt;totam&lt;/h1&gt;</td>\n",
       "      <td>[:h1 \"totam\"]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;h2&gt;&lt;div position=\"415u8Lu6\"&gt;&lt;ul style=\"4lSV13...</td>\n",
       "      <td>[:h2 [:div {:position \"415u8Lu6\"} [:ul {:style...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;li&gt;iustonemoatque&lt;/li&gt;</td>\n",
       "      <td>[:li \"iusto\" \"nemo\" \"atque\"]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;p font=\"lu3gBlCOl3ER97Zi2Cz66H6SsY3\" position...</td>\n",
       "      <td>[:p {:font \"lu3gBlCOl3ER97Zi2Cz66H6SsY3\", :pos...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 xml  \\\n",
       "0                                     <h1>totam</h1>   \n",
       "1  <h2><div position=\"415u8Lu6\"><ul style=\"4lSV13...   \n",
       "2                            <li>iustonemoatque</li>   \n",
       "3  <p font=\"lu3gBlCOl3ER97Zi2Cz66H6SsY3\" position...   \n",
       "\n",
       "                                             closure  \n",
       "0                                      [:h1 \"totam\"]  \n",
       "1  [:h2 [:div {:position \"415u8Lu6\"} [:ul {:style...  \n",
       "2                       [:li \"iusto\" \"nemo\" \"atque\"]  \n",
       "3  [:p {:font \"lu3gBlCOl3ER97Zi2Cz66H6SsY3\", :pos...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_dataset():\n",
    "    with tf.gfile.GFile(\"dataset.tsv\", \"r\") as f:\n",
    "        df = pd.read_csv(f, sep=\"\\t\", header=None, names=[\"xml\", \"closure\"])\n",
    "        return df\n",
    "\n",
    "df = read_dataset()\n",
    "df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate which characters are used in the dataset\n",
    "strs = df[\"xml\"].str.cat(df[\"closure\"])\n",
    "chars = sorted(set(\"\".join(strs.values.tolist())))\n",
    "chars.insert(0, \"<END>\")\n",
    "chars.insert(0, \"\")\n",
    "\n",
    "int_to_char = { idx: ch for idx, ch in enumerate(chars) }\n",
    "char_to_int = { ch: idx for idx, ch in int_to_char.items() }\n",
    "\n",
    "# Calculate maximum string length\n",
    "max_str_len = max(df[\"closure\"].str.len().max(), df[\"xml\"].str.len().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_serie(serie, max_str_len):\n",
    "    # Encoded sequences. Each character encoded to int.\n",
    "    encoded = np.zeros(shape=(len(df), max_str_len), dtype=np.int32)\n",
    "    # Sequnce lengths\n",
    "    str_lens = np.zeros(shape=(len(df),), dtype=np.int32)\n",
    "    # Encode\n",
    "    for idx, single_str in enumerate(serie):\n",
    "        str_lens[idx] = len(single_str)\n",
    "        for j, char in enumerate(single_str):\n",
    "            encoded[idx, j]   = char_to_int[char]\n",
    "    return encoded, str_lens\n",
    "\n",
    "closure_encoded, closure_lens = encode_serie(df[\"closure\"], max_str_len)\n",
    "xml_encoded, xml_lens = encode_serie(df[\"xml\"], max_str_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[:h1 \"totam\"]\n",
      "<h1>totam</h1>\n"
     ]
    }
   ],
   "source": [
    "# Check if the encoding is correct\n",
    "def decode(arr):\n",
    "    return \"\".join([ int_to_char[idx] for idx in arr])\n",
    "\n",
    "print(decode(closure_encoded[0]))\n",
    "print(decode(xml_encoded[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(input_seq, input_lens, target_seq, target_lens, batch_size):\n",
    "    # Truncate dataset, so we'll have complete batches. Otherwise last batch won't be complete.\n",
    "    elems_count = batch_size * (len(input_seq) // batch_size)\n",
    "    for idx in range(0, elems_count, batch_size):\n",
    "        yield input_seq[idx : idx + batch_size], input_lens[idx : idx + batch_size], target_seq[idx : idx + batch_size], target_lens[idx : idx + batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_units = 64\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "# == Inputs ==\n",
    "\n",
    "input_seq = tf.placeholder(tf.int32, shape=(None, max_str_len), name=\"input_seq\")\n",
    "input_seq_lens = tf.placeholder(tf.int32, shape=(None,), name=\"input_seq_lens\")\n",
    "\n",
    "target_seq = tf.placeholder(tf.int32, shape=(None, max_str_len), name=\"target_seq\")\n",
    "target_seq_lens = tf.placeholder(tf.int32, shape=(None,), name=\"target_seq_lens\")\n",
    "\n",
    "\n",
    "# == Encoder ==\n",
    "\n",
    "char_ids = list(int_to_char.keys())\n",
    "encoder_input = tf.one_hot(indices=input_seq,\n",
    "                           depth=len(char_ids),\n",
    "                           name=\"encoder_input_one_hot\")\n",
    "\n",
    "with tf.variable_scope(\"encoder\"):\n",
    "    encoder_cell = tf.contrib.rnn.BasicLSTMCell(num_units)\n",
    "    encoder_initial_state = encoder_cell.zero_state(batch_size, tf.float32)\n",
    "    _, encoder_state = tf.nn.dynamic_rnn(encoder_cell,\n",
    "                                         encoder_input,\n",
    "                                         sequence_length=input_seq_lens,\n",
    "                                         initial_state=encoder_initial_state)\n",
    "\n",
    "    \n",
    "# == Decoder ==\n",
    "\n",
    "trigger_idx = char_to_int[\"<END>\"]\n",
    "trigger_t = tf.constant(trigger_idx, shape=(batch_size, 1))\n",
    "\n",
    "decoder_input_indices = tf.concat([trigger_t, target_seq], axis=1, name=\"decoder_input_indices\")\n",
    "decoder_input = tf.one_hot(indices=decoder_input_indices,\n",
    "                          depth=len(char_ids),\n",
    "                          name=\"decoder_input_one_hot\")\n",
    "decoder_lens = tf.add(target_seq_lens, tf.constant(1))\n",
    "\n",
    "with tf.variable_scope(\"decoder\"):\n",
    "    decoder_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units)\n",
    "    decoder_outputs, decoder_state = tf.nn.dynamic_rnn(decoder_cell,\n",
    "                                                       decoder_input,\n",
    "                                                       sequence_length=decoder_lens,\n",
    "                                                       dtype=tf.float32,\n",
    "                                                       initial_state=encoder_state)\n",
    "\n",
    "logits = tf.layers.dense(decoder_outputs, len(char_ids), name=\"dense\")\n",
    "output = tf.argmax(logits, axis=2, output_type=tf.int32, name=\"argmax\")\n",
    "\n",
    "# Loss\n",
    "\n",
    "decoder_target_indices = tf.concat([target_seq, trigger_t], axis=1, name=\"decoder_target_indices\")\n",
    "decoder_target_one_hot = tf.one_hot(indices=decoder_target_indices,\n",
    "                                    depth=len(char_ids),\n",
    "                                    name=\"decoder_target_one_hot\")\n",
    "\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=decoder_target_one_hot,\n",
    "                                                        logits=logits,\n",
    "                                                        name=\"cross_entropy\")\n",
    "loss_op = tf.reduce_mean(cross_entropy, name=\"loss_op\")\n",
    "\n",
    "# copy-paste from intro-to-RNN\n",
    "grad_clip = 5\n",
    "tvars = tf.trainable_variables()\n",
    "grads, _ = tf.clip_by_global_norm(tf.gradients(loss_op, tvars), grad_clip)\n",
    "train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "optimizer_op = train_op.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "\n",
    "# Summaries\n",
    "\n",
    "tf.summary.histogram(\"cross_entropy/histogram\", cross_entropy)\n",
    "tf.summary.scalar(\"cross_entropy/mean\", loss_op)\n",
    "merged_summary = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "\n",
    "counter = 0\n",
    "save_every_n = 100\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    summary_writer = tf.summary.FileWriter(\"logs/\", sess.graph)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Epoch: {}\".format(epoch))\n",
    "        for xx, xx_lens, yy, yy_lens in get_batches(closure_encoded, closure_lens, xml_encoded, xml_lens, batch_size):\n",
    "            \n",
    "            loss, _, summary = sess.run([loss_op,\n",
    "                                         optimizer_op,\n",
    "                                         merged_summary,\n",
    "                                        ],\n",
    "                                        feed_dict={input_seq: xx,\n",
    "                                                   input_seq_lens: xx_lens,\n",
    "                                                   target_seq: yy,\n",
    "                                                   target_seq_lens: yy_lens,\n",
    "                                                  })\n",
    "            summary_writer.add_summary(summary, counter)\n",
    "            counter += 1\n",
    "            if (counter % save_every_n == 0):\n",
    "                saver.save(sess, \"checkpoints/i_{}.ckpt\".format(counter))\n",
    "                \n",
    "    saver.save(sess, \"checkpoints/i_{}.ckpt\".format(counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/i_1000.ckpt\n",
      "[array([[17, 62, 63, ...,  0,  0,  0],\n",
      "       [17, 59, 63, ...,  0,  0,  0],\n",
      "       [17, 68, 68, ...,  0,  0,  0],\n",
      "       ...,\n",
      "       [17, 62, 63, ...,  0,  0,  0],\n",
      "       [17, 68, 68, ...,  0,  0,  0],\n",
      "       [17, 68, 68, ...,  0,  0,  0]], dtype=int32)]\n"
     ]
    }
   ],
   "source": [
    "checkpoint = tf.train.latest_checkpoint('checkpoints')\n",
    "\n",
    "trigger_idx = char_to_int[\"<END>\"]\n",
    "empty_str = char_to_int[\"\"]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, checkpoint)\n",
    "    \n",
    "    result = np.zeros_like(xml_encoded[0:batch_size])\n",
    "    result.fill(empty_str)\n",
    "    result_len = np.zeros_like(xml_lens[0:batch_size])\n",
    "    result_len[:] = 0\n",
    "    for idx in range(20):\n",
    "        predicted = sess.run([output], feed_dict={\n",
    "            input_seq: closure_encoded[0:batch_size],\n",
    "            input_seq_lens: closure_lens[0:batch_size],\n",
    "            target_seq: result,\n",
    "            target_seq_lens: result_len,\n",
    "        })\n",
    "        result_len[:] += 1\n",
    "        trigger_coords = np.argwhere(predicted == trigger_idx)\n",
    "        for c in trigger_coords:\n",
    "            print(c[0])\n",
    "            result_len[c[0]] = c[1]\n",
    "        \n",
    "    print(predicted)\n",
    "    #xml_idx = np.argmax(xml_idx_logits, axis=1)\n",
    "    #print(xml_idx.shape)\n",
    "    #xml_idx = np.expand_dims(xml_idx, axis=0)\n",
    "    #print(xml_idx.shape)\n",
    "    #s = \"\".join([int_to_char[idx] for idx in xml_idx])\n",
    "    #print(s)\n",
    "    #decode(xml_idx)\n",
    "#    txt = [in xml_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:\n",
      "[:h1 \"totam\"]\n",
      "predicted:\n",
      "<oppaaaapppppppppppp\n",
      "expected:\n",
      "<h1>totam</h1>\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "print(\"input:\")\n",
    "print(decode(closure_encoded[i]))\n",
    "print(\"predicted:\")\n",
    "print(decode(predicted[0][i]))\n",
    "print(\"expected:\")\n",
    "print(decode(xml_encoded[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
